{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "current_directory = os.getcwd()\n",
    "if not os.path.exists(current_directory + '\\\\experimental_data'):\n",
    "    os.makedirs(current_directory + '\\\\experimental_data')\n",
    "    os.makedirs(current_directory + '\\\\experimental_data\\\\plots')\n",
    "    os.makedirs(current_directory + '\\\\experimental_data\\\\results')\n",
    "sys.path.append(current_directory.replace('experiments', ''))\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.set_loglevel('WARNING')\n",
    "from run import execute_simulation, create_experiment_snapshots, execute_ai_training\n",
    "from mtdnetwork.mtd.completetopologyshuffle import CompleteTopologyShuffle\n",
    "from mtdnetwork.mtd.ipshuffle import IPShuffle\n",
    "from mtdnetwork.mtd.hosttopologyshuffle import HostTopologyShuffle\n",
    "from mtdnetwork.mtd.portshuffle import PortShuffle\n",
    "from mtdnetwork.mtd.osdiversity import OSDiversity\n",
    "from mtdnetwork.mtd.servicediversity import ServiceDiversity\n",
    "from mtdnetwork.mtd.usershuffle import UserShuffle\n",
    "from mtdnetwork.mtd.osdiversityassignment import OSDiversityAssignment\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static simulator settings\n",
    "# Environment and agent settings\n",
    "state_size = 8  # HCR, Exposed Endpoints, Attack Success Rate, Attack Path Exposure Score, Return on Attack Score, Attack Path Variability, Risk, Attacker Type\n",
    "time_series_size = 3  # Time Since Last MTD, MTTC, mtd_freqency\n",
    "action_size = 5  # Deploy or don't deploy MTD technique  \n",
    "\n",
    "# Simulator Settings\n",
    "start_time = 0\n",
    "finish_time = 4000\n",
    "mtd_interval = 200\n",
    "scheme = 'mtd_ai'\n",
    "total_nodes = 100\n",
    "new_network = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Set 1: Faster learning with a focus on short-term rewards.\n",
    "\n",
    "create_experiment_snapshots([25, 50, 75, 100])\n",
    "\n",
    "gamma = 0.90  # discount rate\n",
    "epsilon = 1.0  # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.990\n",
    "batch_size = 64\n",
    "train_start = 500\n",
    "episodes = 100\n",
    "\n",
    "# Filename\n",
    "filename = 'parameter_set_1'\n",
    "\n",
    "execute_ai_training(start_time=start_time, finish_time=finish_time, mtd_interval=mtd_interval, state_size=state_size, \n",
    "                    time_series_size=time_series_size, action_size=action_size, gamma=gamma, epsilon=epsilon, \n",
    "                    epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=batch_size, train_start=train_start, \n",
    "                    scheme=scheme, total_nodes=total_nodes, new_network=new_network, episodes=episodes, file_name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Set 2: Focus on long-term rewards with prolonged exploration\n",
    "\n",
    "create_experiment_snapshots([25, 50, 75, 100])\n",
    "\n",
    "gamma = 0.98  # discount rate\n",
    "epsilon = 1.0  # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.997\n",
    "batch_size = 32\n",
    "train_start = 1500\n",
    "episodes = 100\n",
    "\n",
    "# Filename\n",
    "filename = 'parameter_set_2'\n",
    "\n",
    "execute_ai_training(start_time=start_time, finish_time=finish_time, mtd_interval=mtd_interval, state_size=state_size, \n",
    "                    time_series_size=time_series_size, action_size=action_size, gamma=gamma, epsilon=epsilon, \n",
    "                    epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=batch_size, train_start=train_start, \n",
    "                    scheme=scheme, total_nodes=total_nodes, new_network=new_network, episodes=episodes, file_name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Set 3: Early convergence with a smaller batch size.\n",
    "\n",
    "create_experiment_snapshots([25, 50, 75, 100])\n",
    "\n",
    "gamma = 0.92  # discount rate\n",
    "epsilon = 1.0  # exploration rate\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.992\n",
    "batch_size = 16\n",
    "train_start = 750\n",
    "episodes = 100\n",
    "\n",
    "# Filename\n",
    "filename = 'parameter_set_3'\n",
    "\n",
    "execute_ai_training(start_time=start_time, finish_time=finish_time, mtd_interval=mtd_interval, state_size=state_size, \n",
    "                    time_series_size=time_series_size, action_size=action_size, gamma=gamma, epsilon=epsilon, \n",
    "                    epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=batch_size, train_start=train_start, \n",
    "                    scheme=scheme, total_nodes=total_nodes, new_network=new_network, episodes=episodes, file_name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Set 4: Higher initial exploration with a gradual transition.\n",
    "\n",
    "create_experiment_snapshots([25, 50, 75, 100])\n",
    "\n",
    "gamma = 0.95  # discount rate\n",
    "epsilon = 1.2  # exploration rate\n",
    "epsilon_min = 0.02\n",
    "epsilon_decay = 0.996\n",
    "batch_size = 32\n",
    "train_start = 1000\n",
    "episodes = 100\n",
    "\n",
    "# Filename\n",
    "filename = 'parameter_set_4'\n",
    "\n",
    "execute_ai_training(start_time=start_time, finish_time=finish_time, mtd_interval=mtd_interval, state_size=state_size, \n",
    "                    time_series_size=time_series_size, action_size=action_size, gamma=gamma, epsilon=epsilon, \n",
    "                    epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=batch_size, train_start=train_start, \n",
    "                    scheme=scheme, total_nodes=total_nodes, new_network=new_network, episodes=episodes, file_name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Set 5: Robust learning with a large batch size and extended episodes.\n",
    "\n",
    "create_experiment_snapshots([25, 50, 75, 100])\n",
    "\n",
    "gamma = 0.97  # discount rate\n",
    "epsilon = 1.0  # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 128\n",
    "train_start = 2000\n",
    "episodes = 100\n",
    "\n",
    "# Filename\n",
    "filename = 'parameter_set_5'\n",
    "\n",
    "execute_ai_training(start_time=start_time, finish_time=finish_time, mtd_interval=mtd_interval, state_size=state_size, \n",
    "                    time_series_size=time_series_size, action_size=action_size, gamma=gamma, epsilon=epsilon, \n",
    "                    epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=batch_size, train_start=train_start, \n",
    "                    scheme=scheme, total_nodes=total_nodes, new_network=new_network, episodes=episodes, file_name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Set 6: High focus on long-term rewards with rapid transition to exploitation.\n",
    "\n",
    "create_experiment_snapshots([25, 50, 75, 100])\n",
    "\n",
    "gamma = 0.99  # discount rate\n",
    "epsilon = 0.8  # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.985\n",
    "batch_size = 64\n",
    "train_start = 1500\n",
    "episodes = 100\n",
    "\n",
    "# Filename\n",
    "filename = 'parameter_set_6'\n",
    "\n",
    "execute_ai_training(start_time=start_time, finish_time=finish_time, mtd_interval=mtd_interval, state_size=state_size, \n",
    "                    time_series_size=time_series_size, action_size=action_size, gamma=gamma, epsilon=epsilon, \n",
    "                    epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=batch_size, train_start=train_start, \n",
    "                    scheme=scheme, total_nodes=total_nodes, new_network=new_network, episodes=episodes, file_name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Set 7: Emphasis on immediate rewards with high initial exploration.\n",
    "\n",
    "create_experiment_snapshots([25, 50, 75, 100])\n",
    "\n",
    "gamma = 0.85  # discount rate\n",
    "epsilon = 1.2  # exploration rate\n",
    "epsilon_min = 0.02\n",
    "epsilon_decay = 0.990\n",
    "batch_size = 32\n",
    "train_start = 1000\n",
    "episodes = 100\n",
    "\n",
    "# Filename\n",
    "filename = 'parameter_set_7'\n",
    "\n",
    "execute_ai_training(start_time=start_time, finish_time=finish_time, mtd_interval=mtd_interval, state_size=state_size, \n",
    "                    time_series_size=time_series_size, action_size=action_size, gamma=gamma, epsilon=epsilon, \n",
    "                    epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=batch_size, train_start=train_start, \n",
    "                    scheme=scheme, total_nodes=total_nodes, new_network=new_network, episodes=episodes, file_name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Set 8: Long-term planning with delayed training and extended episodes.\n",
    "\n",
    "create_experiment_snapshots([25, 50, 75, 100])\n",
    "\n",
    "gamma = 0.94  # discount rate\n",
    "epsilon = 1.0  # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.997\n",
    "batch_size = 64\n",
    "train_start = 2000\n",
    "episodes = 100\n",
    "\n",
    "# Filename\n",
    "filename = 'parameter_set_8'\n",
    "\n",
    "execute_ai_training(start_time=start_time, finish_time=finish_time, mtd_interval=mtd_interval, state_size=state_size, \n",
    "                    time_series_size=time_series_size, action_size=action_size, gamma=gamma, epsilon=epsilon, \n",
    "                    epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=batch_size, train_start=train_start, \n",
    "                    scheme=scheme, total_nodes=total_nodes, new_network=new_network, episodes=episodes, file_name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Set 9: Robust learning with a large batch size and extended episodes.\n",
    "\n",
    "create_experiment_snapshots([25, 50, 75, 100])\n",
    "\n",
    "gamma = 0.96  # discount rate\n",
    "epsilon = 1.5  # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.980\n",
    "batch_size = 32\n",
    "train_start = 500\n",
    "episodes = 100\n",
    "\n",
    "# Filename\n",
    "filename = 'parameter_set_9'\n",
    "\n",
    "execute_ai_training(start_time=start_time, finish_time=finish_time, mtd_interval=mtd_interval, state_size=state_size, \n",
    "                    time_series_size=time_series_size, action_size=action_size, gamma=gamma, epsilon=epsilon, \n",
    "                    epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=batch_size, train_start=train_start, \n",
    "                    scheme=scheme, total_nodes=total_nodes, new_network=new_network, episodes=episodes, file_name=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Set 10: Robust learning with a large batch size and extended episodes.\n",
    "\n",
    "create_experiment_snapshots([25, 50, 75, 100])\n",
    "\n",
    "gamma = 0.97  # discount rate\n",
    "epsilon = 1.0  # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 32\n",
    "train_start = 3000\n",
    "episodes = 100\n",
    "\n",
    "# Filename\n",
    "filename = 'parameter_set_10'\n",
    "\n",
    "execute_ai_training(start_time=start_time, finish_time=finish_time, mtd_interval=mtd_interval, state_size=state_size, \n",
    "                    time_series_size=time_series_size, action_size=action_size, gamma=gamma, epsilon=epsilon, \n",
    "                    epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=batch_size, train_start=train_start, \n",
    "                    scheme=scheme, total_nodes=total_nodes, new_network=new_network, episodes=episodes, file_name=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mtdsimtime')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b477274c87fb63319f36b484dd34a6881cb387ce1b14309c24558f54c8e02ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
